import os
import plistlib
import re
import sys
import pandas as pd
import tldextract as tld
import ipaddress
import socket
import numpy as np

def find_hostnames(df: pd.DataFrame) -> pd.DataFrame:
    """
    Find hostnames for IPs without a hostname in the given DataFrame.
    
    Args:
        df (pd.DataFrame): DataFrame containing the column 'dst_action_external_hostname' and 'action_remote_ip'.

    Returns:
        pd.DataFrame: DataFrame with IPs and their corresponding hostnames.
    """
    ips_without_hostname = df[df['dst_action_external_hostname'].isna()]['action_remote_ip'].unique()
    sorted_ips = sorted(ips_without_hostname, key=lambda ip: ipaddress.ip_address(ip))

    results = []
    for ip in sorted_ips:
        try:
            hostname = socket.gethostbyaddr(ip)
            results.append({'IP': ip, 'Hostname': hostname[0]})
        except socket.herror:
            results.append({'IP': ip, 'Hostname': "No DNS Record found"})

    return pd.DataFrame(results)


def merge_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    """
    Merge two DataFrames on 'action_remote_ip' and 'IP' columns and update 'dst_action_external_hostname'.

    Args:
        df1 (pd.DataFrame): First DataFrame.
        df2 (pd.DataFrame): Second DataFrame with 'IP' and 'Hostname' columns.

    Returns:
        pd.DataFrame: Merged DataFrame with updated 'dst_action_external_hostname'.
    """
    merged_df = df1.merge(df2, left_on='action_remote_ip', right_on='IP', how='left', right_index=False, left_index=False)

    merged_df['dst_action_external_hostname'] = merged_df.apply(
        lambda row: row['Hostname'] if pd.isna(row['dst_action_external_hostname']) else row['dst_action_external_hostname'],
        axis=1
    )

    merged_df.drop(['Hostname', 'IP'], axis=1, inplace=True)

    return merged_df


def is_ip(address: str):
    """
    Given an Address, verifies if it is an IP or not.

    Args:
        - address (str): The address to be verified.

    Returns:
        - bool: True if the address is an IP (IPv4 or IPv6) or else False.
    """

    try:
        ipaddress.ip_address(address)
        return True
    except ValueError:
        return False


def get_domain_ip(ip: str) -> str:
    """
    Extract the URL of a given IP.

    Args:
        - ip (str): IP that will have an URL extracted from.
        
    Returns:
        - str: Extracted URL.
    """
    try:
        return socket.gethostbyaddr(ip)[0]
    except (socket.gaierror, socket.herror):
        return "URL could not be retrieved"


def extract_main_domain(hostname: str) -> str:
    """
    Extract the main domain from a given hostname.

    Args:
        hostname (str): Hostname to extract the main domain from.

    Returns:
        str: Extracted main domain.
    """
    extracted = tld.extract(hostname)
    if extracted.suffix:
        return f"{extracted.domain}.{extracted.suffix}"
    else:
        return hostname


def extract_main_subdomain(hostname: str) -> str:
    """
    Extract the main subdomain from a given hostname.

    Args:
        hostname (str): Hostname to extract the main subdomain from.

    Returns:
        str: Extracted main subdomain.
    """
    extracted = tld.extract(hostname)
    if extracted.suffix:
        return f"{extracted.subdomain}.{extracted.domain}.{extracted.suffix}"
    else:
        return hostname


def extract_registered_domain(hostname: str) -> str:
    """
    Extract the registered domain from a given hostname.

    Args:
        hostname (str): Hostname to extract the registered domain from.

    Returns:
        str: Extracted registered domain.
    """
    extracted = tld.extract(hostname)
    if extracted.registered_domain:
        return extracted.registered_domain
    else:
        return hostname
  

def get_domain(df: pd.DataFrame) -> pd.DataFrame:
    """
    Adds columns for main domain, main subdomain and registered domain to the DataFrame.

    Parameters:
        df (pd.DataFrame): DataFrame containing column 'dst_action_external_hostname'.

    Returns:
        pd.DataFrame: DataFrame with additional columns 'main_domain', 'main_subdomain' and 'registered_domain'.
    """

    if df['dst_action_external_hostname'].isna().sum() > 0:
        raise ValueError("A coluna de hostnames nÃ£o deve conter valores nulos.")
    
    df['main_domain'] = df['dst_action_external_hostname'].apply(extract_main_domain)
    df['main_subdomain'] = df['dst_action_external_hostname'].apply(extract_main_subdomain)
    df['registered_domain'] = df['dst_action_external_hostname'].apply(extract_registered_domain)
    
    return df


def generate_table_html_hostnames(freq: pd.DataFrame, hostnames: dict, caption: str, column: str) -> str:
    """
    Generate HTML table for collumn frequency with hostnames.

    Args:
        freq (pd.DataFrame): DataFrame containing frequency of IPs.
        hostnames (dict): Dictionary mapping IPs to hostnames.
        caption (str): Caption for the HTML table.
        column (str): Column name.

    Returns:
        str: HTML table as a string.
    """  
    table_html = f"""
    <table border="1" cellspacing="0" cellpadding="5">
        <caption>{caption}</caption>
        <tr>
        <th>{column}</th>
        <th>Hostnames</th>
        <th>Counting Host</th>
        <th>Occurrences Log</th>
        </tr>
    """
    for index, row in freq.iterrows():
        content = row[column]
        hostname = ', '.join(hostnames.get(content, set()))
        counting_host = len(hostname.split(', '))
        occurences = row['count']
        table_html += f"<tr><td>{content}</td><td>{hostname}</td><td>{counting_host}</td><td>{occurences}</td></tr>"
    table_html += "</table>"
    
    return table_html


def generate_table_html(freq: pd.Series, caption: str, column: str) -> str:
    """
    Generate HTML table for frequency of a given column.

    Args:
        freq (pd.Series): Series containing frequency of values.
        caption (str): Caption for the HTML table.
        column (str): Name of the column.

    Returns:
        str: HTML table as a string.
    """
    table_html = f"""
    <table border="1" cellspacing="0" cellpadding="5">
        <caption>{caption}</caption>
        <tr>
        <th>{column}</th>
        <th>Frequency</th>
        </tr>
    """
    for index, value in freq.items():
        table_html += f"<tr><td>{index}</td><td>{value}</td></tr>"
    table_html += "</table>"

    return table_html


def save_frequency_table_hostnames_to_html(df: pd.DataFrame, column: str, file_path: str) -> str:
    """
    Save the ip frequency table in the DataFrame in an HTML file.
    
    Args:
        df (pd.DataFrame): DataFrame containing the IP column.
        column (str): Column name.
        file_path (str): Path to save the HTML file.

    Returns:
        None
    """
    hostnames = df.groupby(column)['dst_action_external_hostname'].apply(lambda x: set(x.dropna().astype(str))).to_dict()

    freq = df[column].value_counts().reset_index()
    freq.columns = [column, 'count']

    filtered_column = df[column].value_counts()
    print(f"Total of {column}: {filtered_column.count()}")
    
    table_html_ip = generate_table_html_hostnames(freq, hostnames, f"Frequency Table - {column}", column)
    with open(file_path, 'w') as file:
        file.write(table_html_ip)


def save_frequency_table_to_html(df: pd.DataFrame, column: str, file_path: str):
    """
    Save the frequency table of a given column in the DataFrame to an HTML file.

    Args:
        df (pd.DataFrame): DataFrame containing the column.
        column (str): Name of the column.
        file_path (str): Path to save the HTML file.

    Returns:
        None
    """
    filtered_column = df[column].value_counts()
    print(f"Total of {column}: {filtered_column.count()}")
    
    table_html = generate_table_html(filtered_column, f"Frequency Table - {column}", column)
    
    with open(file_path, 'w') as file:
        file.write(table_html)


def is_application_in_path(path: str, app: str) -> bool:
    """
    Check if the application string is present in the given path string.

    Args:
        path (str): The path string to search within.
        app (str): The application string to search for.

    Returns:
        bool: True if the application string is found in the path string, False otherwise.
    """
    return app.lower() in path.lower()


def filter_app(df: pd.DataFrame, app: str, columns: list[str] = None) -> pd.DataFrame:
    """
    Filter a DataFrame to include only rows where a specified application string is found in the given columns or in the 'causality_actor_process_image_path' column.

    Args:
        df (pd.DataFrame): The DataFrame to be filtered.
        app (str): The application string to search for.
        columns (list[str], optional): List of column names to search within. Defaults to None, which means all columns are searched.

    Returns:
        pd.DataFrame: A new DataFrame containing only the rows where the application string is found in the specified columns or in the 'causality_actor_process_image_path' column.

    Raises:
        ValueError: If none of the specified columns are found in the DataFrame.
    """
    if columns is None:
        columns = df.columns.tolist()

    valid_columns = [col for col in columns if col in df.columns]
    if not valid_columns:
        raise ValueError("No valid search columns found in DataFrame.")

    if 'causality_actor_process_image_path' in df.columns:
        app_filtered_df = df[df['causality_actor_process_image_path'].apply(lambda x: is_application_in_path(x, app))]

    return app_filtered_df[columns]


def save_csv (df: pd.DataFrame, output_file: str):
    """
    Saves a DataFrame to a CSV file.

    Parameters:
    - df (pd.DataFrame): The DataFrame to be saved.
    - output_file (str): The path to the output CSV file.

    Returns:
    - None
    """
    df.to_csv(output_file, index=False)


def save_apps_csv(df: pd.DataFrame, applications: list[str], directory: str):
    """
    Saves a filtered version of the DataFrame to CSV files for each application.

    This function iterates over a list of applications, filters the DataFrame for each application,
    and saves the filtered DataFrame to a CSV file named after the application in the specified directory.

    Parameters:
    - df (pd.DataFrame): The DataFrame to be filtered and saved.
    - applications (list[str]): A list of application names to filter the DataFrame.
    - directory (str): The directory where the CSV files will be saved.

    Returns:
    - None
    """
    for app in applications:
        filtered_df = filter_app(df, app)
        output_file = directory + f'{app}.csv'
        if len(filtered_df) > 1:
            save_csv(filtered_df, output_file)

    
def is_standard_application(path: str, applications: list[str]) -> bool:
    """
    Checks if any of the standard application names are present in the given file path.

    Parameters:
    - path (str): The file path to check.
    - applications (list[str]): A list of standard application names to look for.

    Returns:
    - bool: True if any of the application names are found in the file path, False otherwise.
    """
    return any(app.lower() in path.lower() for app in applications)


def filter_standard_applications(df: pd.DataFrame, applications: list[str]) -> pd.DataFrame:
    """
    Identifies and filters standard applications in a DataFrame based on process image paths.

    Parameters:
    - df (pd.DataFrame): The DataFrame containing the following columns:
        - `os_actor_process_image_path` (str): File path for the OS actor process image.
        - `causality_actor_process_image_path` (str): File path for the causality actor process image.
    - applications (list[str]): A list of standard application names to look for.

    Returns:
    - pd.DataFrame: A DataFrame filtered to only include rows where both 
      `os_actor_process_image_path` and `causality_actor_process_image_path` correspond to standard applications.
    """
    df['is_standard_application'] = df.apply(
        lambda row: (
            is_standard_application(row['os_actor_process_image_path'], applications) and
            is_standard_application(row['causality_actor_process_image_path'], applications)
        ),
        axis=1
    )

    return df[df['is_standard_application']]


def find_app_to_domain(df: pd.DataFrame, apps: list[str]) -> pd.DataFrame:
    """
    Associates registered domains with standard applications based on process image paths.

    Parameters:
    - df (pd.DataFrame): The DataFrame containing the following columns:
        - `registered_domain` (str): The registered domain.
        - `os_actor_process_image_path` (str): File path for the OS actor process image.
    - apps (list[str]): A list of standard application names to look for.

    Returns:
    - pd.DataFrame: A DataFrame containing the registered domains and associated applications,
      where each row represents a domain and its associated applications.
    """
    results = []

    for domain in df['registered_domain'].unique():
        domain_df = df[df['registered_domain'] == domain]
        
        associated_apps = []
        for app in apps:
            if domain_df['os_actor_process_image_path'].apply(lambda x: is_application_in_path(x, app)).any():
                associated_apps.append(app)
        
        results.append({
            'registered_domain': domain,
            'dst_action_external_hostname': ', '.join(associated_apps)
        })

    return pd.DataFrame(results)


def combination_host_and_path(df: pd.DataFrame) -> pd.DataFrame:
    """
    Filters and removes duplicate combinations of process image paths and registered domains.

    Parameters:
    - df (pd.DataFrame): The DataFrame containing the following columns:
        - `causality_actor_process_image_path` (str): The path of the causality actor process image.
        - `registered_domain` (str): The registered domain.
    
    Returns:
    - pd.DataFrame: A DataFrame containing unique combinations of process image paths and registered domains,
      excluding records where `registered_domain` is 'No DNS Record found'.
    """

    df = df[df['registered_domain'] != 'No DNS Record found']
    
    unique_df = df.drop_duplicates(subset=['causality_actor_process_image_path', 'registered_domain'])
    
    return unique_df


def get_domains(endpoints: list[str], ip_urls_dict: dict) -> list[str]:
    """
    Given a list of endpoints, converts them to unique domains and returns a list of these domains.

    Args:
    - endpoints (list[str]): A list of endpoints.
    - ip_urls_dict (dict): A dictionary that maps an IP to its respective URL.

    Returns:
    - list[str]: A list of unique domains extracted from the endpoins given.
    """
    domains_set = set()
    
    for entry in endpoints:
        if is_ip(entry):
            if entry in ip_urls_dict:
                respective_url = ip_urls_dict[entry]
            else:
                respective_url = get_domain_ip(entry)
                ip_urls_dict[entry] = respective_url
            
            if respective_url != "URL could not be retrieved" and respective_url != "_gateway":
                domains_set.add(extract_main_domain(respective_url))
        else:
            domains_set.add(extract_main_domain(entry))

    return list(domains_set)


def find_contents_path(path: str) -> str:
    """
    Finds the last subpath ending with '/Contents/' in the given string.

    Args:
    - path (str): The input path string.

    Returns:
    - str: The matched subpath, or "unknown" if no match is found.
    """
    match = re.match(r'^(.*Contents/)', path)
    return match.group(1) if match else "unknown"


def read_sv_file(input_sv_file_path: str) -> pd.DataFrame:
    """
    Process a CSV or TSV file to generate a corresponding Pandas DataFrame.

    Parameters:
    - input_sv_file_path (str): The file path to the input CSV or TSV file.

    Returns:
    - pd.DataFrame: A DataFrame containing all of the information given by the C/TSV file
      or None if the given file isn't a CSV or TSV.  
    """
    if re.match(r'^.*\.csv$', input_sv_file_path) is not None:
        return pd.read_csv(input_sv_file_path)
    elif re.match(r'^.*\.tsv$', input_sv_file_path) is not None:
        return pd.read_csv(input_sv_file_path, sep='\t')
    else:
        return None   


def get_identifier(executable_path: str) -> str:
    """"
    Given an executable path, returns its Identifier or a "unknown" string when
    it could not be retrieved â for example, if the path does not exist locally.

    Args:
    - executable_path (str): The full path to the executable whose identifier 
      needs to be retrieved. 

    Returns:
    - str: The identifier of the executable if found, otherwise, "unknown".
    """
    app_bundle_path = find_contents_path(executable_path)
    plist_path = os.path.join(app_bundle_path, "Info.plist")
    
    if os.path.exists(plist_path):
        with open(plist_path, 'rb') as plist_file:
            plist_data = plistlib.load(plist_file)
            return plist_data.get("CFBundleIdentifier", "unknown")
    else:
        return "unknown"


def simplify_destinations(dest_list):
    """
    Given a list of destinations, simplifies the list by replacing all occurrences of endpoints that
    appear 100 times or more with a single destination that has the specific endpoint and the keyword
    "any" as the port. Endpoints with fewer than 100 destinations associated with it are preserved
    in their original form.

    Args:
    - dest_list (list of tuples): A list of destinations, where each destination is a tuple of (endpoint, port).
      The `endpoint` is the first element of the tuple and represents the destination address (e.g., IP or hostname).
      The `port` is the second element of the tuple and represents the destination port.

    Returns:
    - list of tuples: A simplified list of destinations, where:
        - If an endpoint appears 100 or more times in the input list, all occurrences of that endpoint are replaced
          with a single tuple: (endpoint, "any").
        - If an endpoint appears fewer than 100 times, all its original (endpoint, port) tuples are preserved.
    """
    endpoint_counts = {}
    for endpoint, _ in dest_list:
        endpoint_counts[endpoint] = endpoint_counts.get(endpoint, 0) + 1

    simplified_dest_list = []
    simplified_endpoints = set()
    for endpoint, port in dest_list:
        if endpoint_counts[endpoint] >= 100 and endpoint not in simplified_endpoints:
            simplified_dest_list.append((endpoint, "any"))
            simplified_endpoints.add(endpoint)
        elif endpoint_counts[endpoint] < 100:
            if (endpoint, port) not in simplified_dest_list:
                simplified_dest_list.append((endpoint, port))

    return simplified_dest_list


def filtered_df_to_intermediate_csv(filtered_df: pd.DataFrame, is_simplified: bool, output_csv_path: str) -> None:
    """
    Convert a dataframe produced from cortex logs to an intermediate CSV on rules-catalog/data/

    This intermediate csv and its file path will be both be created at runtime if it wasn't
    created in a previous execution, and then a processed dataframe will be converted to this
    CSV.

    Args:
    - filtered_df (pd.Dataframe): Dataframe created from cortex logs with only the following columns:
      'causality_actor_process_image_path', 'action_remote_ip', 'action_remote_port' and 'dst_action_external_hostname'.
    - is_simplified (bool): Boolean value that determines if endpoints with >99 occurrences of destinations
      will be simplified to (endpoint, "any").
    - output_csv_path (str): The file path where the output CSV file will be saved.

    Returns:
    - None: The function writes the results directly to the specified output CSV file.
    
    Output CSV Structure:
    - causality_actor_process_image_path (str): The unique process image paths found in the original SV files.
    - destinations (set): A set of lists consisting of unique values from the 'action_remote_ip' and 
      'dst_action_external_hostname' columns with their associated 'action_remote_port' value.
    - identifier (str): The unique identifier corresponding to the causality_actor_process_image_path.
    """
    def get_destinations(group):
        destinations = set()
        for _, row in group.iterrows():
            port = row['action_remote_port']
            if pd.isna(port):
                port = "any"
            else:
                port = int(port)

            if pd.notna(row['action_remote_ip']):
                destinations.add((row['action_remote_ip'], str(port)))
            if pd.notna(row['dst_action_external_hostname']):
                destinations.add((row['dst_action_external_hostname'], str(port)))

        return list(destinations)
    
    result = filtered_df.groupby('causality_actor_process_image_path').apply(get_destinations).reset_index()

    result.columns = ['causality_actor_process_image_path', 'destinations']
    
    if is_simplified:
        result['destinations'] = result['destinations'].apply(simplify_destinations)

    if 'identifier' not in filtered_df.columns:
        result['identifier'] = result['causality_actor_process_image_path'].apply(get_identifier)
    else:
        result = result.merge(filtered_df[['causality_actor_process_image_path', 'identifier']], on='causality_actor_process_image_path', how='left')

    result.to_csv(output_csv_path, index=False)


def process_block_file(input_block_file_path: str, is_simplified: bool, output_csv_path: str) -> None:
    """
    Processes a CSV file that has an app to be blocked per line and possibly its respective identifier
    to generate a new CSV with unique subpaths and associated destinations and identifier.

    This function reads an input CSV file and for each line creates a line on a dataframe with the 
    respective columns: 'causality_actor_process_image_path', 'action_remote_ip', 'action_remote_port' 
    and 'dst_action_external_hostname', 'identiier', after that it sends this dataframe to an auxiliary
    function to write this dataframe to a new CSV file.

    Parameters:
    - input_block_file_path (str): The file path to the input CSV file.
    - is_simplified (bool): Boolean value that determines if the rules archive will be simplified
      or not.
    - output_csv_path (str): The file path where the output CSV file will be saved.
    
    Returns:
    - None: The function writes the results directly to the specified output CSV file.
    
    Output CSV Structure:
    - causality_actor_process_image_path (str): The unique process image subpaths based on 'app' column 
      of the input csv file.
    - destinations (set): A set of lists consisting of unique values from the 'action_remote_ip' and 
      'dst_action_external_hostname' columns with their associated 'action_remote_port' value.
    - identifier (str): The unique identifier corresponding to the causality_actor_process_image_path.
    """
    absolute_path = os.path.expanduser(input_block_file_path)

    df = pd.read_csv(absolute_path)

    if not {'app', 'identifier'}.issubset(df.columns):
        raise ValueError("Input CSV must contain 'app' and 'identifier' columns.")

    df['identifier'] = df['identifier'].fillna("unknown")

    data = {
        'causality_actor_process_image_path': df['app'],
        'action_remote_ip': [np.nan] * len(df),
        'action_remote_port': [np.nan] * len(df),
        'dst_action_external_hostname': ['any'] * len(df),
        'identifier': df['identifier']
    }

    df = pd.DataFrame(data)

    filtered_df_to_intermediate_csv(df, is_simplified, output_csv_path)


def process_sv_file(input_sv_file_path: str, is_simplified: bool, output_csv_path: str) -> None:
    """
    Processes a CSV or TSV file to generate a new CSV with unique paths and associated destinations and 
    identifier.

    This function reads an input CSV or TSV file, filters and groups data by the unique paths found 
    in the 'causality_actor_process_image_path' column, and creates a set of associated destinations
    from the 'action_remote_ip', 'dst_action_external_hostname' and 'action_remote_port' columns. 
    'action_remote_ip', 'dst_action_external_hostname' with null or blank values are excluded. 
    The result is saved to a new CSV file.

    Parameters:
    - input_sv_file_path (str): The file path to the input CSV or TSV file.
    - is_simplified (bool): Boolean value that determines if the rules archive will be simplified
      or not.
    - output_csv_path (str): The file path where the output CSV file will be saved.
    
    Returns:
    - None: The function writes the results directly to the specified output CSV file.
    
    Output CSV Structure:
    - causality_actor_process_image_path (str): The unique process image paths found in the original SV files.
    - destinations (set): A set of lists consisting of unique values from the 'action_remote_ip' and 
      'dst_action_external_hostname' columns with their associated 'action_remote_port' value.
    - identifier (str): The unique identifier corresponding to the causality_actor_process_image_path.
    """
    absolute_path = os.path.expanduser(input_sv_file_path)

    df = read_sv_file(absolute_path)
    
    if df is None:
        print('The given file format is not supported, only CSV or TSV')
        sys.exit()

    df_filtered = df[['causality_actor_process_image_path', 'action_remote_ip', 'action_remote_port', 'dst_action_external_hostname']]
    df_filtered = df_filtered.dropna(subset=['action_remote_ip', 'dst_action_external_hostname'], how='all')

    filtered_df_to_intermediate_csv(df_filtered, is_simplified, output_csv_path)


def process_sv_directory(input_sv_directory_path: str, is_simplified: bool, output_csv_path: str) -> None:
    """
    Processes a directory of CSV or TSV files to generate a new CSV with unique paths and associated destinations
    and identifier.

    This function reads an input directory, filters and groups data of its logs by the unique 
    paths found in the 'causality_actor_process_image_path' column, and creates a set of associated 
    destinations from the 'action_remote_ip', 'dst_action_external_hostname' and 'action_remote_port' columns.
    'action_remote_ip' and 'dst_action_external_hostname' with null or blank values are excluded. 
    The result is saved to a new CSV file.

    Parameters:
    - input_sv_directory_path (str): The file path to the input directory of logs.
    - is_simplified (bool): Boolean value that determines if the rules archive will be simplified
      or not.
    - output_csv_path (str): The file path where the output CSV file will be saved.
    
    Returns:
    - None: The function writes the results directly to the specified output CSV file.
    
    Output CSV Structure:
    - causality_actor_process_image_path (str): The unique process image paths found in the original SV files 
      directory.
    - destinations (set): A set of lists consisting of unique values from the 'action_remote_ip' and 
      'dst_action_external_hostname' columns with their associated 'action_remote_port' value.
    - identifier (str): The unique identifier corresponding to the causality_actor_process_image_path.
    """
    absolute_path = os.path.expanduser(input_sv_directory_path)

    if not os.path.isdir(absolute_path):
        print("The given directory doesn't exist")
        sys.exit()

    df = None

    directory = os.scandir(absolute_path)
    for entry in directory :
        if entry.is_file() and df is None:
            df = read_sv_file(absolute_path + entry.name)
        elif entry.is_file() and df is not None:
            df_to_concat = read_sv_file(absolute_path + entry.name)
            
            if df_to_concat is not None:
                df = pd.concat([df, df_to_concat])

    if df is None:
        print("There is no CSV or TSV file inside the given directory")
        sys.exit()

    df_filtered = df[['causality_actor_process_image_path', 'action_remote_ip', 'action_remote_port', 'dst_action_external_hostname']]
    df_filtered = df_filtered.dropna(subset=['action_remote_ip', 'dst_action_external_hostname'], how='all')

    filtered_df_to_intermediate_csv(df_filtered, is_simplified, output_csv_path)